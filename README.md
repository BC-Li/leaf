<!-- <h1 align="center">🍃Leaf</h1>
<div align="center"> -->
  This repo contains my paper reading notes on deep learning and some toy project code, kaggle writeups etc.
  Check the notes at https://bc-li.github.io/paperreading.
</div>

## Paper reading notes
### PHASE #1
| Title                                                        | Field | Time | Report link                       | Time I started | Status      |
| ------------------------------------------------------------ | ----- | ---- | --------------------------------- | ----------- | ----------- |
| [ICCV 2015] Learning Deconvolution Network for Semantic Segmentation | VISION   | 2015 | https://bc-li.github.io/paper/deconvnet | 2021/5/17 | Done |
| [NeurIPS 2017] Attention Is All You Need                     | NLP   | 2017 | https://bc-li.github.io/paper/transformer | 2021/12/11 | Done |
| [NAACL 2019] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | NLP   | 2018 | https://bc-li.github.io/paper/bert         | 2021/12/15 | Done       |
| [NeurIPS 2014] Sequence to Sequence Learning with Neural Networks | NLP   | 2014 | https://bc-li.github.io/paper/seq2seq      | 2022/1/21 | Done  |
| [ICLR 2018] Non-Autoregressive Neural Machine Translation | NLP | 2018 | https://bc-li.github.io/paper/nonauto | 2022/1/24 | Done |
| [ICLR 2019] Parameter-Efficient Transfer Learning for NLP | NLP | 2019 | https://bc-li.github.io/paper/petl | 2022/2/2 | Done |
| [ICLR 2018] Unsupervised Neural Machine Translation | NLP | 2018 | https://bc-li.github.io/paper/unsupervised-NMT | 2022/2/4 | Done |
### PHASE #2
| Title                                                        | Field | Time | Report link                       | Time I started | Status      |
| ------------------------------------------------------------ | ----- | ---- | --------------------------------- | ----------- | ----------- |
| [NeurIPS 2019] Levenshtein Transformer | NLP | 2019 | https://bc-li.github.io/paper/lt | 2022/2/15 | Pending |

### Stack

| Title                                                        | Field | Time | Report link                       | Time I started | Status      |
| ------------------------------------------------------------ | ----- | ---- | --------------------------------- | ----------- | ----------- |
| [SCTS 2020] Pre-trained Models for Natural Language Processing: A Survey | NLP   | 2020 | N/A | N/A | N/A |

> 写 blog 的时候如未特殊说明则为从约为零基础开始。在 blog post 中我会把我为了理解文中一些比较 specific 的概念找到的相对容易理解的原出处贴到文中，方便查阅，且不再重复阐述。

## Future release plan
* Pytorch code corresponding to the papers
* Kaggle writeups
* Course projects (will release at 2022 summer)

## Acknowledgements

https://github.com/mli/paper-reading

https://www.deeplearningbook.org/

